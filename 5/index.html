<html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>CS180 Project 5</title><style>
/* cspell:disable-file */
/* webkit printing magic: print all background colors */
html {
	-webkit-print-color-adjust: exact;
}
* {
	box-sizing: border-box;
	-webkit-print-color-adjust: exact;
}

html,
body {
	margin: 0;
	padding: 0;
}
@media only screen {
	body {
		margin: 2em auto;
		max-width: 900px;
		color: rgb(55, 53, 47);
	}
}

body {
	line-height: 1.5;
	white-space: pre-wrap;
}

a,
a.visited {
	color: inherit;
	text-decoration: underline;
}

.pdf-relative-link-path {
	font-size: 80%;
	color: #444;
}

h1,
h2,
h3 {
	letter-spacing: -0.01em;
	line-height: 1.2;
	font-weight: 600;
	margin-bottom: 0;
}

.page-title {
	font-size: 2.5rem;
	font-weight: 700;
	margin-top: 0;
	margin-bottom: 0.75em;
}

h1 {
	font-size: 1.875rem;
	margin-top: 1.875rem;
}

h2 {
	font-size: 1.5rem;
	margin-top: 1.5rem;
}

h3 {
	font-size: 1.25rem;
	margin-top: 1.25rem;
}

.source {
	border: 1px solid #ddd;
	border-radius: 3px;
	padding: 1.5em;
	word-break: break-all;
}

.callout {
	border-radius: 3px;
	padding: 1rem;
}

figure {
	margin: 1.25em 0;
	page-break-inside: avoid;
}

figcaption {
	opacity: 0.5;
	font-size: 85%;
	margin-top: 0.5em;
}

mark {
	background-color: transparent;
}

.indented {
	padding-left: 1.5em;
}

hr {
	background: transparent;
	display: block;
	width: 100%;
	height: 1px;
	visibility: visible;
	border: none;
	border-bottom: 1px solid rgba(55, 53, 47, 0.09);
}

img {
	max-width: 100%;
}

@media only print {
	img {
		max-height: 100vh;
		object-fit: contain;
	}
}

@page {
	margin: 1in;
}

.collection-content {
	font-size: 0.875rem;
}

.column-list {
	display: flex;
	justify-content: space-between;
}

.column {
	padding: 0 1em;
}

.column:first-child {
	padding-left: 0;
}

.column:last-child {
	padding-right: 0;
}

.table_of_contents-item {
	display: block;
	font-size: 0.875rem;
	line-height: 1.3;
	padding: 0.125rem;
}

.table_of_contents-indent-1 {
	margin-left: 1.5rem;
}

.table_of_contents-indent-2 {
	margin-left: 3rem;
}

.table_of_contents-indent-3 {
	margin-left: 4.5rem;
}

.table_of_contents-link {
	text-decoration: none;
	opacity: 0.7;
	border-bottom: 1px solid rgba(55, 53, 47, 0.18);
}

table,
th,
td {
	border: 1px solid rgba(55, 53, 47, 0.09);
	border-collapse: collapse;
}

table {
	border-left: none;
	border-right: none;
}

th,
td {
	font-weight: normal;
	padding: 0.25em 0.5em;
	line-height: 1.5;
	min-height: 1.5em;
	text-align: left;
}

th {
	color: rgba(55, 53, 47, 0.6);
}

ol,
ul {
	margin: 0;
	margin-block-start: 0.6em;
	margin-block-end: 0.6em;
}

li > ol:first-child,
li > ul:first-child {
	margin-block-start: 0.6em;
}

ul > li {
	list-style: disc;
}

ul.to-do-list {
	padding-inline-start: 0;
}

ul.to-do-list > li {
	list-style: none;
}

.to-do-children-checked {
	text-decoration: line-through;
	opacity: 0.375;
}

ul.toggle > li {
	list-style: none;
}

ul {
	padding-inline-start: 1.7em;
}

ul > li {
	padding-left: 0.1em;
}

ol {
	padding-inline-start: 1.6em;
}

ol > li {
	padding-left: 0.2em;
}

.mono ol {
	padding-inline-start: 2em;
}

.mono ol > li {
	text-indent: -0.4em;
}

.toggle {
	padding-inline-start: 0em;
	list-style-type: none;
}

/* Indent toggle children */
.toggle > li > details {
	padding-left: 1.7em;
}

.toggle > li > details > summary {
	margin-left: -1.1em;
}

.selected-value {
	display: inline-block;
	padding: 0 0.5em;
	background: rgba(206, 205, 202, 0.5);
	border-radius: 3px;
	margin-right: 0.5em;
	margin-top: 0.3em;
	margin-bottom: 0.3em;
	white-space: nowrap;
}

.collection-title {
	display: inline-block;
	margin-right: 1em;
}

.page-description {
    margin-bottom: 2em;
}

.simple-table {
	margin-top: 1em;
	font-size: 0.875rem;
	empty-cells: show;
}
.simple-table td {
	height: 29px;
	min-width: 120px;
}

.simple-table th {
	height: 29px;
	min-width: 120px;
}

.simple-table-header-color {
	background: rgb(247, 246, 243);
	color: black;
}
.simple-table-header {
	font-weight: 500;
}

time {
	opacity: 0.5;
}

.icon {
	display: inline-block;
	max-width: 1.2em;
	max-height: 1.2em;
	text-decoration: none;
	vertical-align: text-bottom;
	margin-right: 0.5em;
}

img.icon {
	border-radius: 3px;
}

.user-icon {
	width: 1.5em;
	height: 1.5em;
	border-radius: 100%;
	margin-right: 0.5rem;
}

.user-icon-inner {
	font-size: 0.8em;
}

.text-icon {
	border: 1px solid #000;
	text-align: center;
}

.page-cover-image {
	display: block;
	object-fit: cover;
	width: 100%;
	max-height: 30vh;
}

.page-header-icon {
	font-size: 3rem;
	margin-bottom: 1rem;
}

.page-header-icon-with-cover {
	margin-top: -0.72em;
	margin-left: 0.07em;
}

.page-header-icon img {
	border-radius: 3px;
}

.link-to-page {
	margin: 1em 0;
	padding: 0;
	border: none;
	font-weight: 500;
}

p > .user {
	opacity: 0.5;
}

td > .user,
td > time {
	white-space: nowrap;
}

input[type="checkbox"] {
	transform: scale(1.5);
	margin-right: 0.6em;
	vertical-align: middle;
}

p {
	margin-top: 0.5em;
	margin-bottom: 0.5em;
}

.image {
	border: none;
	margin: 1.5em 0;
	padding: 0;
	border-radius: 0;
	text-align: center;
}

.code,
code {
	background: rgba(135, 131, 120, 0.15);
	border-radius: 3px;
	padding: 0.2em 0.4em;
	border-radius: 3px;
	font-size: 85%;
	tab-size: 2;
}

code {
	color: #eb5757;
}

.code {
	padding: 1.5em 1em;
}

.code-wrap {
	white-space: pre-wrap;
	word-break: break-all;
}

.code > code {
	background: none;
	padding: 0;
	font-size: 100%;
	color: inherit;
}

blockquote {
	font-size: 1.25em;
	margin: 1em 0;
	padding-left: 1em;
	border-left: 3px solid rgb(55, 53, 47);
}

.bookmark {
	text-decoration: none;
	max-height: 8em;
	padding: 0;
	display: flex;
	width: 100%;
	align-items: stretch;
}

.bookmark-title {
	font-size: 0.85em;
	overflow: hidden;
	text-overflow: ellipsis;
	height: 1.75em;
	white-space: nowrap;
}

.bookmark-text {
	display: flex;
	flex-direction: column;
}

.bookmark-info {
	flex: 4 1 180px;
	padding: 12px 14px 14px;
	display: flex;
	flex-direction: column;
	justify-content: space-between;
}

.bookmark-image {
	width: 33%;
	flex: 1 1 180px;
	display: block;
	position: relative;
	object-fit: cover;
	border-radius: 1px;
}

.bookmark-description {
	color: rgba(55, 53, 47, 0.6);
	font-size: 0.75em;
	overflow: hidden;
	max-height: 4.5em;
	word-break: break-word;
}

.bookmark-href {
	font-size: 0.75em;
	margin-top: 0.25em;
}

.sans { font-family: ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol"; }
.code { font-family: "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace; }
.serif { font-family: Lyon-Text, Georgia, ui-serif, serif; }
.mono { font-family: iawriter-mono, Nitti, Menlo, Courier, monospace; }
.pdf .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK JP'; }
.pdf:lang(zh-CN) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC'; }
.pdf:lang(zh-TW) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK TC'; }
.pdf:lang(ko-KR) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK KR'; }
.pdf .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.pdf .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK JP'; }
.pdf:lang(zh-CN) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK SC'; }
.pdf:lang(zh-TW) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK TC'; }
.pdf:lang(ko-KR) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK KR'; }
.pdf .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.highlight-default {
	color: rgba(55, 53, 47, 1);
}
.highlight-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.highlight-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.highlight-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.highlight-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.highlight-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.highlight-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.highlight-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.highlight-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.highlight-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.highlight-default_background {
	color: rgba(55, 53, 47, 1);
}
.highlight-gray_background {
	background: rgba(241, 241, 239, 1);
}
.highlight-brown_background {
	background: rgba(244, 238, 238, 1);
}
.highlight-orange_background {
	background: rgba(251, 236, 221, 1);
}
.highlight-yellow_background {
	background: rgba(251, 243, 219, 1);
}
.highlight-teal_background {
	background: rgba(237, 243, 236, 1);
}
.highlight-blue_background {
	background: rgba(231, 243, 248, 1);
}
.highlight-purple_background {
	background: rgba(244, 240, 247, 0.8);
}
.highlight-pink_background {
	background: rgba(249, 238, 243, 0.8);
}
.highlight-red_background {
	background: rgba(253, 235, 236, 1);
}
.block-color-default {
	color: inherit;
	fill: inherit;
}
.block-color-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.block-color-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.block-color-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.block-color-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.block-color-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.block-color-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.block-color-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.block-color-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.block-color-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.block-color-default_background {
	color: inherit;
	fill: inherit;
}
.block-color-gray_background {
	background: rgba(241, 241, 239, 1);
}
.block-color-brown_background {
	background: rgba(244, 238, 238, 1);
}
.block-color-orange_background {
	background: rgba(251, 236, 221, 1);
}
.block-color-yellow_background {
	background: rgba(251, 243, 219, 1);
}
.block-color-teal_background {
	background: rgba(237, 243, 236, 1);
}
.block-color-blue_background {
	background: rgba(231, 243, 248, 1);
}
.block-color-purple_background {
	background: rgba(244, 240, 247, 0.8);
}
.block-color-pink_background {
	background: rgba(249, 238, 243, 0.8);
}
.block-color-red_background {
	background: rgba(253, 235, 236, 1);
}
.select-value-color-uiBlue { background-color: rgba(35, 131, 226, .07); }
.select-value-color-pink { background-color: rgba(245, 224, 233, 1); }
.select-value-color-purple { background-color: rgba(232, 222, 238, 1); }
.select-value-color-green { background-color: rgba(219, 237, 219, 1); }
.select-value-color-gray { background-color: rgba(227, 226, 224, 1); }
.select-value-color-transparentGray { background-color: rgba(227, 226, 224, 0); }
.select-value-color-translucentGray { background-color: rgba(0, 0, 0, 0.06); }
.select-value-color-orange { background-color: rgba(250, 222, 201, 1); }
.select-value-color-brown { background-color: rgba(238, 224, 218, 1); }
.select-value-color-red { background-color: rgba(255, 226, 221, 1); }
.select-value-color-yellow { background-color: rgba(253, 236, 200, 1); }
.select-value-color-blue { background-color: rgba(211, 229, 239, 1); }
.select-value-color-pageGlass { background-color: undefined; }
.select-value-color-washGlass { background-color: undefined; }

.checkbox {
	display: inline-flex;
	vertical-align: text-bottom;
	width: 16;
	height: 16;
	background-size: 16px;
	margin-left: 2px;
	margin-right: 5px;
}

.checkbox-on {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20width%3D%2216%22%20height%3D%2216%22%20fill%3D%22%2358A9D7%22%2F%3E%0A%3Cpath%20d%3D%22M6.71429%2012.2852L14%204.9995L12.7143%203.71436L6.71429%209.71378L3.28571%206.2831L2%207.57092L6.71429%2012.2852Z%22%20fill%3D%22white%22%2F%3E%0A%3C%2Fsvg%3E");
}

.checkbox-off {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20x%3D%220.75%22%20y%3D%220.75%22%20width%3D%2214.5%22%20height%3D%2214.5%22%20fill%3D%22white%22%20stroke%3D%22%2336352F%22%20stroke-width%3D%221.5%22%2F%3E%0A%3C%2Fsvg%3E");
}
	
</style></head><body><article id="1443b13b-f27a-8030-b29a-f8de7dcdf258" class="page sans"><header><h1 class="page-title">CS180 Project 5</h1><p class="page-description"></p></header><div class="page-body"><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="1443b13b-f27a-80c1-a2a5-f3204cc823be"><div style="font-size:1.5em"><span class="icon">🛠</span></div><div style="width:100%"><h3 id="3fae4266-3e5b-46b7-a2f1-7035dad0fd38" class="">Vivek Bharati</h3></div></figure><h1 id="1443b13b-f27a-8091-b868-c6a031e7a40f" class="">Goal</h1><p id="1443b13b-f27a-80f5-b4d0-db5d43d1af59" class="">The goal of this assignment is to experiment with diffusion models, from implementation to testing out various techniques.</p><h1 id="1443b13b-f27a-80a2-aed7-d7bd091a37ce" class="">Section A</h1><p id="1443b13b-f27a-808b-9bf6-ef0fab1128b8" class="">In this section, I was tasked with playing around with a <a href="https://huggingface.co/docs/diffusers/api/pipelines/deepfloyd_if">pre-trained diffusion model</a>. To start off, I displayed random model output for three pre-computed text embeddings. I generated two sets of images, the first one with 20 inference steps, and the second one with 30 inference steps.</p><p id="1443b13b-f27a-808e-acdd-e52484ba5ba0" class=""> Set 1:</p><figure id="1443b13b-f27a-8053-911e-ce62da2ad719" class="image"><a href="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_9.06.18_PM.png"><img style="width:708px" src="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_9.06.18_PM.png"/></a><figcaption>num_inference_steps=20</figcaption></figure><p id="1443b13b-f27a-808b-8a13-cf9b48541f5c" class="">Set 2:</p><figure id="1443b13b-f27a-80e6-8290-efc215fc729d" class="image"><a href="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_9.11.10_PM.png"><img style="width:708px" src="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_9.11.10_PM.png"/></a><figcaption>num_inference_steps=30</figcaption></figure><p id="1443b13b-f27a-8047-985e-c0b1cb8038ff" class="">Both sets of images appear to be quite high quality, albeit a bit cartoonish for the rocket ship generated images. It is intriguing that the same pose was captured in both images of “a man wearing a hat,” although the color filter was different between the two. </p><p id="1443b13b-f27a-80ef-8cbe-f11857d8a2cd" class="">
</p><p id="1443b13b-f27a-80e4-8fa4-dc6a90559874" class="">Note: For all random operations in Section A, I used a seed of 108. </p><h2 id="1443b13b-f27a-800f-8edb-c51d563d8be6" class="">Part 1</h2><p id="1443b13b-f27a-800a-9967-d8784ef66bcb" class="">In this part, I implemented code to add random, normally distributed noise to an image. This code would be useful for later parts, where I add noise to an image, and the model should learn how to denoise an image. </p><div id="1443b13b-f27a-806d-b0b8-e4bdd81cfcb8" class="column-list"><div id="1443b13b-f27a-809b-baff-d87c27758428" style="width:25%" class="column"><figure id="1443b13b-f27a-8002-896f-d519d978a648" class="image"><a href="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_8.41.04_PM.png"><img style="width:132px" src="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_8.41.04_PM.png"/></a><figcaption>input image</figcaption></figure></div><div id="1443b13b-f27a-8009-8dc4-f84ce27af34a" style="width:25%" class="column"><figure id="1443b13b-f27a-803e-9d5e-e944c5c79057" class="image"><a href="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_8.41.54_PM.png"><img style="width:128px" src="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_8.41.54_PM.png"/></a><figcaption>noise at t = 250</figcaption></figure></div><div id="1443b13b-f27a-8002-ae20-f1a4471dc3b0" style="width:25%" class="column"><figure id="1443b13b-f27a-8015-a407-e9ea167fe90f" class="image"><a href="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_8.44.04_PM.png"><img style="width:132px" src="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_8.44.04_PM.png"/></a><figcaption>noise at t = 500</figcaption></figure></div><div id="1443b13b-f27a-8017-adcd-c231944827fc" style="width:25%" class="column"><figure id="1443b13b-f27a-80e1-963a-f079ddc3480c" class="image"><a href="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_8.42.17_PM.png"><img style="width:130px" src="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_8.42.17_PM.png"/></a><figcaption>noise at t = 750</figcaption></figure></div></div><p id="1443b13b-f27a-805d-b190-cc3eded9a680" class="">Here, t stands for the time step. Higher t values indicate more noise is added to the image.</p><p id="1443b13b-f27a-803d-850d-e59506848f21" class="">
</p><p id="1443b13b-f27a-8073-b480-c6cf88f127ef" class="">Before using the diffusion model to denoise an image, I tried using a simple gaussian blur technique to smooth out the image (take away high frequencies present in the image): </p><p id="1443b13b-f27a-805d-ac16-cca0c2fe1b34" class="">
</p><p id="1443b13b-f27a-80cc-b462-e6dd20b02e82" class="">
</p><div id="1443b13b-f27a-8002-a50e-e3addf3e6c16" class="column-list"><div id="1443b13b-f27a-8037-8d8a-e753f73c5a16" style="width:33.333333333333336%" class="column"><figure id="1443b13b-f27a-8070-b884-e092190bf286" class="image"><a href="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_8.41.54_PM.png"><img style="width:128px" src="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_8.41.54_PM.png"/></a><figcaption>noise at t = 250</figcaption></figure></div><div id="1443b13b-f27a-801d-af23-e8f9bea2fee1" style="width:33.333333333333336%" class="column"><figure id="1443b13b-f27a-80ca-a912-f22af1af68ba" class="image"><a href="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_8.44.04_PM.png"><img style="width:132px" src="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_8.44.04_PM.png"/></a><figcaption>noise at t = 500</figcaption></figure></div><div id="1443b13b-f27a-809e-b337-d7a9b4c701f1" style="width:33.33333333333333%" class="column"><figure id="1443b13b-f27a-808c-a99f-d72c42a48f78" class="image"><a href="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_8.42.17_PM.png"><img style="width:130px" src="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_8.42.17_PM.png"/></a><figcaption>noise at t = 750</figcaption></figure></div></div><div id="1443b13b-f27a-80a8-8783-fbb6b963225d" class="column-list"><div id="1443b13b-f27a-802e-ada5-ea848ceb13a8" style="width:33.333333333333336%" class="column"><figure id="1443b13b-f27a-80cc-98a0-c8251c5e1b1d" class="image"><a href="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_8.46.55_PM.png"><img style="width:130px" src="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_8.46.55_PM.png"/></a><figcaption>denoised (t = 250)</figcaption></figure><p id="1443b13b-f27a-80fd-a7a2-d2f92537446c" class="">
</p></div><div id="1443b13b-f27a-80f7-9358-f488887bc526" style="width:33.333333333333336%" class="column"><figure id="1443b13b-f27a-80db-81da-ea425906901f" class="image"><a href="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_8.47.13_PM.png"><img style="width:134px" src="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_8.47.13_PM.png"/></a><figcaption>denoised (t = 500)</figcaption></figure><p id="1443b13b-f27a-80a0-ac2d-f72df88d6159" class="">
</p></div><div id="1443b13b-f27a-8093-a8e8-f4e9e08b71c5" style="width:33.33333333333333%" class="column"><figure id="1443b13b-f27a-8089-98a9-f92a28200567" class="image"><a href="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_8.47.24_PM.png"><img style="width:132px" src="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_8.47.24_PM.png"/></a><figcaption>denoised (t = 750)</figcaption></figure></div></div><p id="1443b13b-f27a-802b-9009-e9c168360fcc" class="">As seen above, this method did not work well, hence the need for diffusion models.</p><p id="1443b13b-f27a-80b2-b9f9-deeb670e778b" class="">
</p><p id="1443b13b-f27a-8014-8d5d-d471a1969569" class="">The first method I tested out with the pre-trained diffusion model was one-step denoising. Here, I passed in x_t (the noisy image) and t (the “time step”) to the UNet, in an attempt to denoise the image in one pass. The results are as follows:</p><p id="1443b13b-f27a-80ff-ac24-e01118cf88c6" class="">
</p><figure id="1443b13b-f27a-800a-aab8-f165a5883a7a" class="image"><a href="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_8.53.37_PM.png"><img style="width:266px" src="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_8.53.37_PM.png"/></a><figcaption>Apart from the original image, the noisy image input is on the left, and the denoised model output is on the right</figcaption></figure><p id="1443b13b-f27a-804e-9c9c-f981301d2167" class="">Although this one-step method fared better than the naïve Gaussian blur approach, it still did not produce desirable output. Particularly, for higher values of t, the model output gets progressively further from the ground truth.</p><p id="1443b13b-f27a-80df-8360-ce14eef90127" class="">
</p><p id="1443b13b-f27a-807e-b327-ecb95f38c2d0" class="">One method to solve this issue is iteratively. In other words, the ground truth estimate can be updated over several time steps, starting from mostly noise (t = 990) to (ideally) no noise (t = 0). The results from this method are shown below:</p><p id="1443b13b-f27a-80e6-80d2-c73e85fe05e9" class="">
</p><figure id="1443b13b-f27a-80bc-a248-e888e13a843c" class="image"><a href="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_10.56.58_PM.png"><img style="width:450px" src="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_10.56.58_PM.png"/></a></figure><p id="1443b13b-f27a-8086-bc44-c9a7d40956e6" class="">
</p><p id="1443b13b-f27a-8076-98b4-e5752c6ba87d" class="">Given that we can use the diffusion model to iteratively denoise an image, we can iteratively denoise random, normally distributed noise.  Starting from pure noise allows us to generate images from scratch, with no ground truth to go off of. I generated a sample of 5 images, all with the text prompt “a high quality photo”:</p><p id="1443b13b-f27a-80fe-a44e-fb28e4690392" class="">
</p><figure id="1443b13b-f27a-8077-98c9-f4bf2ea5b730" class="image"><a href="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_9.01.47_PM.png"><img style="width:130px" src="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_9.01.47_PM.png"/></a></figure><p id="1443b13b-f27a-80be-a0e3-ca26b25bf0a6" class="">
</p><p id="1443b13b-f27a-8062-bd5f-e137c1d93ce6" class="">Evidently, these images aren’t very coherent. To fix this, I implemented a process called <a href="https://arxiv.org/abs/2207.12598">Classifier Free Guidance (CFG)</a>. Here, I used the text prompt “a high quality photo” as my conditional prompt, and the null prompt “” as my unconditional prompt. I generated 5 new images using this technique with the guidance scale set to 7:</p><p id="1443b13b-f27a-80d2-8b89-cd2e1b1e9f1c" class="">
</p><figure id="1443b13b-f27a-8091-997b-d64083496720" class="image"><a href="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_9.22.14_PM.png"><img style="width:134px" src="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_9.22.14_PM.png"/></a></figure><p id="1443b13b-f27a-8062-b6f4-d5b080be9d75" class="">
</p><p id="1443b13b-f27a-80c6-863f-f49114ea67c9" class="">Evidently, these images are much higher quality than those produced without CFG. </p><p id="1443b13b-f27a-80fc-bfe8-cb1de45d7bae" class="">
</p><p id="1443b13b-f27a-80f2-a462-ea1cddf29638" class="">Another technique is <a href="https://sde-image-editing.github.io/">SDEdit</a>, where the original image is perturbed with noise. Then, over a series of “edits,” or iterative denoising patterns, the original image is recovered. I tested out this process with varying starting indices (lower starting index means that the starting image is closer to pure noise).</p><p id="1443b13b-f27a-80ac-bfcc-f69538ad5abb" class="">
</p><figure id="1443b13b-f27a-80cd-814e-d686738fd503" class="image"><a href="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_9.29.13_PM.png"><img style="width:340px" src="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_9.29.13_PM.png"/></a></figure><p id="1443b13b-f27a-8049-9536-ded9b47bce82" class="">
</p><p id="1443b13b-f27a-80af-b4cc-c7970a05031d" class="">This process worked decently well and eventually recovered something close to the original image:<br/><br/></p><figure id="1443b13b-f27a-80bd-b960-e91892e22140" class="image"><a href="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_8.41.04_PM.png"><img style="width:132px" src="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_8.41.04_PM.png"/></a><figcaption>input image</figcaption></figure><p id="1443b13b-f27a-804e-88d1-f699d26aac03" class="">
</p><p id="1443b13b-f27a-80fb-bdb6-f3f655f1b098" class="">I tested out this SDEdit method for a few more sets of images. The first one is obtained from the web, and the other two were hand drawn. </p><p id="1443b13b-f27a-801e-92cc-c31e189fd61e" class=""><br/>Set 1 (web image):<br/></p><p id="1443b13b-f27a-8015-9b06-fb1cdd2d8be2" class="">
</p><figure id="1443b13b-f27a-8070-9040-ee6f01530cc4" class="image"><a href="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_9.35.53_PM.png"><img style="width:126px" src="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_9.35.53_PM.png"/></a><figcaption>input</figcaption></figure><p id="1443b13b-f27a-805d-94e2-f1c33a6d30b2" class="">
</p><figure id="1443b13b-f27a-8095-b110-ee4584c8492f" class="image"><a href="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_9.36.51_PM.png"><img style="width:708px" src="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_9.36.51_PM.png"/></a><figcaption>generated images with corresponding starting indices</figcaption></figure><p id="1443b13b-f27a-8003-a59b-cb9571adb65f" class="">
</p><p id="1443b13b-f27a-80ea-8149-fad054764bcc" class="">Set 2 (hand drawn):</p><p id="1443b13b-f27a-8069-a2ff-ed7af2461a4f" class="">
</p><figure id="1443b13b-f27a-80ec-8ee0-df8796102604" class="image"><a href="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_10.59.55_PM.png"><img style="width:134px" src="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_10.59.55_PM.png"/></a><figcaption>input (attempt at McLaren logo)</figcaption></figure><p id="1443b13b-f27a-800b-9dd6-fb7728085876" class="">
</p><figure id="1443b13b-f27a-8057-a3a4-d45e5c1ecae4" class="image"><a href="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_11.00.50_PM.png"><img style="width:708px" src="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_11.00.50_PM.png"/></a><figcaption>generated images with corresponding starting indices</figcaption></figure><p id="1443b13b-f27a-801e-b6ab-d96ec962595c" class="">
</p><p id="1443b13b-f27a-8022-a150-f53f04c70067" class="">Set 3 (hand drawn):</p><figure id="1443b13b-f27a-80c8-a2dc-c4768650d492" class="image"><a href="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_11.02.43_PM.png"><img style="width:144px" src="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_11.02.43_PM.png"/></a><figcaption>input (attempt at apple)</figcaption></figure><p id="1443b13b-f27a-8024-8f14-c9bc81a6804c" class="">
</p><figure id="1443b13b-f27a-8056-966f-e72fdc03a9c2" class="image"><a href="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_11.03.45_PM.png"><img style="width:708px" src="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_11.03.45_PM.png"/></a><figcaption>generated images with corresponding starting indices</figcaption></figure><p id="1443b13b-f27a-8068-b53e-f2c47d4d940a" class="">
</p><p id="1443b13b-f27a-8091-bd4b-cb16c27f2e8f" class="">I also used the diffusion model to perform inpainting, where I filled in an image that had some portions removed based on the test image:</p><p id="1443b13b-f27a-80f5-a06b-fdc83ae966ca" class="">Set 1:</p><p id="1443b13b-f27a-8038-ab95-f628e38df62d" class="">
</p><figure id="1443b13b-f27a-80a9-8699-fa93ee139731" class="image"><a href="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_11.20.50_PM.png"><img style="width:414px" src="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_11.20.50_PM.png"/></a></figure><p id="1443b13b-f27a-80a7-b733-d70854b0b005" class="">
</p><figure id="1443b13b-f27a-805a-8fd2-e5dc9d564434" class="image"><a href="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_11.22.27_PM.png"><img style="width:216px" src="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_11.22.27_PM.png"/></a></figure><p id="1443b13b-f27a-80a6-ab6d-cf403652b757" class="">
</p><p id="1443b13b-f27a-809e-8708-f86898c83521" class="">Set 2 (custom):</p><figure id="1443b13b-f27a-80d2-8f13-c5551dd934f3" class="image"><a href="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_11.43.05_PM.png"><img style="width:412px" src="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_11.43.05_PM.png"/></a></figure><p id="1443b13b-f27a-80dd-bb65-e4b5b023fb44" class="">
</p><p id="1443b13b-f27a-80a5-8305-e9bfabf0e607" class="">Set 3(custom):</p><figure id="1443b13b-f27a-80e7-b80c-fe5b99b11d2a" class="image"><a href="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_11.43.55_PM.png"><img style="width:414px" src="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_11.43.55_PM.png"/></a></figure><p id="1443b13b-f27a-80cf-9c08-f66764219475" class="">
</p><p id="1443b13b-f27a-8080-bd31-ee6c004a1c0a" class="">I also did conditional image-to-image translation, where I started with a prompt of “a rocket ship” and ended with the original image (the Campanile). </p><figure id="1443b13b-f27a-801d-b340-d400f4e03330" class="image"><a href="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_11.05.33_PM.png"><img style="width:144px" src="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_11.05.33_PM.png"/></a></figure><p id="1443b13b-f27a-8014-b265-ee8ff30654cd" class="">
</p><p id="1443b13b-f27a-80d0-81b9-d5f6593953f7" class="">I implemented visual anagrams, where I used one prompt to denoise one orientation of the image, and used another prompt to denoise another orientation of the image. The result is an image that looks like the first prompt when facing up, and looks like the other prompt when facing the other way:</p><p id="1443b13b-f27a-80af-a3a8-e250dbc1f6b2" class="">Set 1:</p><p id="1443b13b-f27a-806b-a346-ff7180533ca5" class="">
</p><figure id="1443b13b-f27a-80e4-acd7-fb3d92153546" class="image"><a href="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_11.27.44_PM.png"><img style="width:134px" src="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_11.27.44_PM.png"/></a><figcaption>top: &quot;an oil painting of people around a campfire”, bottom: &quot;an oil painting of an old man”</figcaption></figure><p id="1443b13b-f27a-8028-8cec-c4907370fa97" class="">Set 2:</p><p id="1443b13b-f27a-80a8-9375-c61b6c464567" class="">
</p><figure id="1443b13b-f27a-80d7-a15b-dcf8a0193970" class="image"><a href="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_11.28.51_PM.png"><img style="width:126px" src="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_11.28.51_PM.png"/></a><figcaption>top: &quot;a lithograph of a skull”, bottom: &quot;a lithograph of waterfalls”</figcaption></figure><p id="1443b13b-f27a-80d5-9fac-efde53730c10" class="">Set 3:</p><p id="1443b13b-f27a-800b-9389-cde2ca1aadae" class="">
</p><figure id="1443b13b-f27a-80a5-9fbb-cb0c389b1d20" class="image"><a href="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_11.30.50_PM.png"><img style="width:134px" src="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_11.30.50_PM.png"/></a><figcaption>top: &quot;a pencil”,  bottom: &quot;a rocket ship”</figcaption></figure><p id="1443b13b-f27a-804e-b35e-d2367b7b6f12" class="">
</p><p id="1443b13b-f27a-80c9-a357-d2fbe3b583a0" class="">Finally, I generated hybrid images (something that changes in appearance based on distance to image). I considered the high frequency noise of one prompt, and the low frequency noise of the other prompt. The results are as follows:</p><p id="1443b13b-f27a-80e4-a5b9-f435a4431c91" class="">
</p><p id="1443b13b-f27a-804b-b78f-cfa0e40b1b85" class="">Set 1:</p><figure id="1443b13b-f27a-800b-bf9b-f3a23ee79ad9" class="image"><a href="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_11.49.01_PM.png"><img style="width:132px" src="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_11.49.01_PM.png"/></a><figcaption>Hybrid image of a skull and a waterfall</figcaption></figure><p id="1443b13b-f27a-8080-8292-ed5be71425c1" class="">
</p><p id="1443b13b-f27a-80d2-95ed-ddff501b8bb0" class="">Set 2:</p><p id="1443b13b-f27a-80bf-b4b7-cf9f46fc97de" class="">
</p><figure id="1443b13b-f27a-801c-aea4-d47e340e4cda" class="image"><a href="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_11.48.31_PM.png"><img style="width:132px" src="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_11.48.31_PM.png"/></a><figcaption>Hybrid image of a snowy mountain village and a rocket ship (see bottom right corner)</figcaption></figure><p id="1443b13b-f27a-8019-ad1e-dc64fc5a12b1" class="">
</p><p id="1443b13b-f27a-8069-b1fe-c1ccffe9cefe" class="">Set 3:</p><figure id="1443b13b-f27a-8015-aee8-daeb12191b48" class="image"><a href="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_11.48.47_PM.png"><img style="width:132px" src="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_11.48.47_PM.png"/></a><figcaption>Hybrid image of a man wearing a hat and an old man</figcaption></figure><p id="1443b13b-f27a-809f-bb53-c1a36bf74614" class="">
</p><h1 id="1443b13b-f27a-8061-b536-e762cbaac1a0" class="">Section B</h1><p id="1443b13b-f27a-80f6-a61d-ffabc4ba9fcd" class="">In this section, I was tasked with training a diffusion model from scratch.</p><p id="1443b13b-f27a-8017-a68f-e70f9ead1cfe" class="">
</p><h2 id="1443b13b-f27a-80e3-8b64-ef19cad8cb88" class="">Part 1</h2><p id="1443b13b-f27a-80ed-8721-f3bf4c757e5b" class="">In this first part, I used a UNet to train a denoiser network on the MNIST dataset. I optimized over the denoising problem, where I minimize the L2 loss between the denoised image and the ground truth image. I visualized the denoising process for various noisy images:</p><p id="1443b13b-f27a-807d-9f73-e1e53fe7af9b" class="">
</p><figure id="1443b13b-f27a-8036-9b0a-dfc38c98626f" class="image"><a href="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_9.46.56_PM.png"><img style="width:300px" src="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_9.46.56_PM.png"/></a></figure><p id="1443b13b-f27a-80d6-803d-c743bf449c8a" class="">Here, higher sigma values correspond to higher levels of noise being added to the original image. The noise, as in Section A, is normally distributed. </p><p id="1443b13b-f27a-8091-a735-c755d4e04146" class="">Next, I trained the UNet denoiser network to denoise images with sigma=0.5. The training cycle involved a batch size of 256, with 5 total epochs. I used a UNet architecture with hidden dimension D = 128, and an Adam optimizer with a learning rate of 1e-04. Below is a plot of my training losses over each batch:</p><p id="1443b13b-f27a-80c2-8f1a-fdc617051957" class="">
</p><figure id="1443b13b-f27a-80e4-b3ab-fe3a5663b0dd" class="image"><a href="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_9.52.05_PM.png"><img style="width:528px" src="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_9.52.05_PM.png"/></a><figcaption>y-axis is loss (mean squared error) between the denoised sample and the ground truth, and the x-axis corresponds to the batch number. This plot is based on the training set data</figcaption></figure><p id="1443b13b-f27a-8017-a19f-e6f8098bd418" class="">Here are results for denoising after 1 epoch of training:</p><p id="1443b13b-f27a-80db-aef0-dd5436432286" class="">
</p><figure id="1443b13b-f27a-80de-bda1-e51464448754" class="image"><a href="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_9.51.52_PM.png"><img style="width:278px" src="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_9.51.52_PM.png"/></a></figure><p id="1443b13b-f27a-8085-a23b-fde3030f6ef8" class="">Below are results for denoising after 5 epochs of training:</p><p id="1443b13b-f27a-8049-b9d7-e9e013d88064" class="">
</p><figure id="1443b13b-f27a-803e-9c59-eabf5a9da6b0" class="image"><a href="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_9.51.38_PM.png"><img style="width:274px" src="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_9.51.38_PM.png"/></a></figure><p id="1443b13b-f27a-80d4-866e-d60eff7a507b" class="">
</p><p id="1443b13b-f27a-80ae-a641-e768119377b3" class="">I also tested the model with out-of-distribution images (passing in noisy images with sigma values not necessarily equal to 0.5):</p><figure id="1443b13b-f27a-803b-8e35-e6cdb101bb78" class="image"><a href="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_10.33.28_PM.png"><img style="width:144px" src="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_10.33.28_PM.png"/></a></figure><h2 id="1443b13b-f27a-802c-a6c4-f7b6bec17c02" class="">Part 2</h2><p id="1443b13b-f27a-8028-95e5-c3f560db9622" class="">In this part, I trained a diffusion model to iteratively denoise images. Specifically, I added time-conditioning to the UNet. Therefore, the model could predict the denoised image given the noisy image and the current time step t. For this training cycle, I used a batch size of 128, with 20 total epochs. I used a UNet architecture with hidden dimension D = 64, and an Adam optimizer with learning rate 1e-03. I also added an exponential learning rate decay scheduler. Below is a plot of my training losses over each batch:</p><p id="1443b13b-f27a-8050-81f0-cb203c7dee16" class="">
</p><figure id="1443b13b-f27a-8007-9711-df762856239d" class="image"><a href="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_10.02.17_PM.png"><img style="width:528px" src="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_10.02.17_PM.png"/></a><figcaption>y-axis is loss (mean squared error) between the iteratively denoised sample and the ground truth, and the x-axis corresponds to the batch number. This plot is based on the training set data</figcaption></figure><p id="1443b13b-f27a-80d2-a3d1-e5d48a7d6a92" class="">
</p><h2 id="1443b13b-f27a-805a-a719-ecee6b395d61" class="">Part 3</h2><p id="1443b13b-f27a-800e-a3c1-ebfb0ff5e85c" class="">Below are my denoising results for select epochs during the training cycle:</p><p id="1443b13b-f27a-80a2-a374-fb49497c5b3b" class="">
</p><figure id="1443b13b-f27a-80fc-9483-e084b7c713ff" class="image"><a href="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_10.03.06_PM.png"><img style="width:602px" src="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_10.03.06_PM.png"/></a><figcaption>epoch 1</figcaption></figure><figure id="1443b13b-f27a-80f5-8f01-eb5cadb3ddac" class="image"><a href="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_10.03.16_PM.png"><img style="width:598px" src="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_10.03.16_PM.png"/></a><figcaption>epoch 5</figcaption></figure><figure id="1443b13b-f27a-8000-a593-dc3293f133b1" class="image"><a href="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_10.03.26_PM.png"><img style="width:598px" src="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_10.03.26_PM.png"/></a><figcaption>epoch 10</figcaption></figure><figure id="1443b13b-f27a-806c-9a63-f113e2417b4b" class="image"><a href="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_10.03.34_PM.png"><img style="width:598px" src="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_10.03.34_PM.png"/></a><figcaption>epoch 15</figcaption></figure><figure id="1443b13b-f27a-8055-825e-ffff3e6377f2" class="image"><a href="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_10.03.42_PM.png"><img style="width:602px" src="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_10.03.42_PM.png"/></a><figcaption>epoch 20</figcaption></figure><p id="1443b13b-f27a-801f-998a-c6941f1789e0" class="">
</p><h2 id="1443b13b-f27a-8080-afb5-d8761fc3a8e0" class="">Part 4</h2><p id="1443b13b-f27a-80ed-850d-c182b0b8cee3" class="">In this part, I trained a diffusion model to iteratively denoise images. Here, I added class-conditioning to the UNet in addition to time-conditioning. Therefore, the model could predict the denoised image based on the class (numeral) of the image. For this training cycle, I used a batch size of 128, with 20 total epochs. I used a UNet architecture with hidden dimension D = 64, and an Adam optimizer with learning rate 1e-03. I also added an exponential learning rate decay scheduler. Below is a plot of my training losses over each batch:</p><figure id="1443b13b-f27a-8094-a665-d9c757eed5d2" class="image"><a href="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_10.08.47_PM.png"><img style="width:528px" src="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_10.08.47_PM.png"/></a><figcaption>y-axis is loss (mean squared error) between the iteratively denoised sample and the ground truth, and the x-axis corresponds to the batch number. This plot is based on the training set data</figcaption></figure><h2 id="1443b13b-f27a-80f5-8c32-d2539d0ad387" class="">Part 5</h2><p id="1443b13b-f27a-80e1-b299-edd38908f476" class="">For this part, I sampled generated images based on all classes with classifier free guidance (CFG). Below are my denoising results for select epochs during the training cycle:</p><p id="1443b13b-f27a-80b9-98d8-f3dd7aad9087" class="">
</p><figure id="1443b13b-f27a-8092-afd4-d878ca9e1a5b" class="image"><a href="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_10.11.31_PM.png"><img style="width:600px" src="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_10.11.31_PM.png"/></a><figcaption>epoch 1</figcaption></figure><figure id="1443b13b-f27a-8073-9bb6-f0407155e387" class="image"><a href="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_10.11.40_PM.png"><img style="width:604px" src="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_10.11.40_PM.png"/></a><figcaption>epoch 5</figcaption></figure><figure id="1443b13b-f27a-8096-b6be-fb34043eed4a" class="image"><a href="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_10.11.50_PM.png"><img style="width:602px" src="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_10.11.50_PM.png"/></a><figcaption>epoch 10</figcaption></figure><figure id="1443b13b-f27a-8044-8fd3-c40f8bccd1ab" class="image"><a href="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_10.11.59_PM.png"><img style="width:602px" src="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_10.11.59_PM.png"/></a><figcaption>epoch 15</figcaption></figure><figure id="1443b13b-f27a-80ae-b0ba-f09fd55d3af9" class="image"><a href="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_10.12.10_PM.png"><img style="width:602px" src="CS180%20Project%205%201443b13bf27a8030b29af8de7dcdf258/Screenshot_2024-11-19_at_10.12.10_PM.png"/></a><figcaption>epoch 20</figcaption></figure><p id="1443b13b-f27a-8068-9f4c-d88cfeeff77b" class="">
</p></div></article><span class="sans" style="font-size:14px;padding-top:2em"></span></body></html>